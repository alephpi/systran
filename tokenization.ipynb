{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "**Warning:** the notebook is only for learning purpose, and the produced model cannot be directly used for OpenNMT-py or OpenNMT-tf\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyonmttok as tok"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# learner = tok.SentencePieceLearner(vocab_size=32000, character_coverage=0.98)\n",
    "tokenizer = tok.Tokenizer(\n",
    "    \"aggressive\", joiner_annotate=True, segment_numbers=True)\n",
    "learner = tok.BPELearner(tokenizer=tokenizer, symbols=32000)\n",
    "learner.ingest_file('./corpus/train.en')\n",
    "learner.ingest_file('./corpus/train.fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'learner' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer \u001b[39m=\u001b[39m learner\u001b[39m.\u001b[39mlearn(\u001b[39m'\u001b[39m\u001b[39m./tokenization-model/tokenizer-32k_en_fr_v2\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'learner' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = learner.learn('./tokenization-model/tokenizer-32k_en_fr_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lst = []\n",
    "\n",
    "with open('./corpus/train.en', encoding='utf-8') as train_file:\n",
    "    for l in train_file:\n",
    "        lst.append(l)\n",
    "with open('./corpus/train.fr', encoding='utf-8') as train_file:\n",
    "    for l in train_file:\n",
    "        lst.append(l)\n",
    "\n",
    "vocab = tok.build_vocab_from_lines(\n",
    "    lst,\n",
    "    tokenizer=tokenizer,\n",
    "    maximum_size=32000,\n",
    "    special_tokens=[\"<blank>\", \"<unk>\", \"<s>\", \"</s>\"],\n",
    ")\n",
    "\n",
    "with open(\"./corpus/vocab_en_fr.txt\", \"w\", encoding='utf-8') as vocab_file:\n",
    "    for token in vocab.ids_to_tokens:\n",
    "        vocab_file.write(\"%s\\n\" % token)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = tok.Tokenizer(\"aggressive\",\n",
    "                          bpe_model_path='./tokenization-model/tokenizer-32k_en_fr_v2',\n",
    "                          joiner_annotate=True,\n",
    "                          segment_numbers=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(\n",
    "    'Try this example! $$ &', training=True, as_token_objects=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Token('&')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[-1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## detokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Try this example! $$ &'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.detokenize(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenize file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.tokenize_file(input_path='./corpus/train.en',\n",
    "                        output_path='./corpus/train.en.tok', num_threads=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.tokenize_file(input_path='./corpus/train.fr',\n",
    "                        output_path='./corpus/train.fr.tok', num_threads=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.tokenize_file(input_path='./corpus/test.en',\n",
    "                        output_path='./corpus/test.en.tok', num_threads=8)\n",
    "tokenizer.tokenize_file(input_path='./corpus/test.fr',\n",
    "                        output_path='./corpus/test.fr.tok', num_threads=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize_file(input_path='./corpus/rep_test.fr',\n",
    "                        output_path='./corpus/rep_test.fr.tok', num_threads=8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## detokenize file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.detokenize_file(\n",
    "    input_path='../corpus/test.en.out.tok.avg', output_path='../corpus/test.en.out.avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.detokenize_file(\n",
    "    input_path='./corpus/naive_penalty_2.fr.tok', output_path='./corpus/naive_penalty_2.fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.detokenize_file(\n",
    "    input_path='./corpus/naive_penalty_3.fr.tok', output_path='./corpus/naive_penalty_3.fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.detokenize_file(input_path='./corpus/naive_penalty_2_mask.fr.tok',\n",
    "                          output_path='./corpus/naive_penalty_2_mask.fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.detokenize_file(input_path='./corpus/naive_penalty_3_mask.fr.tok',\n",
    "                          output_path='./corpus/naive_penalty_3_mask.fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.detokenize_file(input_path='./corpus/naive_penalty_3_penalty_decay_0.9_mask.fr.tok',\n",
    "                          output_path='./corpus/naive_penalty_3_penalty_decay_0.9_mask.fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.detokenize_file(input_path='./corpus/naive_penalty_2_penalty_decay_0.9_mask.fr.tok',\n",
    "                          output_path='./corpus/naive_penalty_2_penalty_decay_0.9_mask.fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.detokenize_file(input_path='./corpus/naive_penalty_10_mask.fr.tok',\n",
    "                          output_path='./corpus/naive_penalty_10_mask.fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.detokenize_file(input_path='./corpus/naive_penalty_100_mask.fr.tok',\n",
    "                          output_path='./corpus/naive_penalty_100_mask.fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.detokenize_file(input_path='./corpus/train-approach/test.en.out.tok.6000',\n",
    "                          output_path='./corpus/train-approach/test.en.out.6000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.detokenize_file(input_path='./corpus/train-approach/test.en.out.tok.avg',\n",
    "                          output_path='./corpus/train-approach/test.en.out.avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.detokenize_file(input_path='./corpus/train-approach/test.en.out.tok.14000',\n",
    "                          output_path='./corpus/train-approach/test.en.out.14000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.detokenize_file(input_path='./corpus/train-approach/test.en.out.tok.9000',\n",
    "                          output_path='./corpus/train-approach/test.en.out.9000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.detokenize_file(input_path='./corpus/train-approach/test.en.out.tok.17000',\n",
    "                          output_path='./corpus/train-approach/test.en.out.17000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.detokenize_file(input_path='./corpus/train-approach/test.en.out.tok.13000',\n",
    "                          output_path='./corpus/train-approach/test.en.out.13000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.detokenize_file(input_path='./corpus/train-approach/test.en.out.tok.729-13000',\n",
    "                          output_path='./corpus/train-approach/test.en.out.729-13000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.detokenize_file(input_path='./corpus/train-approach/test.en.out.tok.731-13000',\n",
    "                          output_path='./corpus/train-approach/test.en.out.731-13000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.detokenize_file(input_path='./corpus/train-approach/rep_test.en.out.tok.731-13000',\n",
    "                          output_path='./corpus/train-approach/rep_test.en.out.731-13000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.detokenize_file(input_path='./corpus/train-approach/rep_test.en.out.tok.729-13000',\n",
    "                          output_path='./corpus/train-approach/rep_test.en.out.729-13000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
